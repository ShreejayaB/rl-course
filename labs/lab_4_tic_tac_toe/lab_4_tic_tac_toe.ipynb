{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-431f49c1a6d7fbf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tic-tac-toe-with-Q-learning\" data-toc-modified-id=\"Tic-tac-toe-with-Q-learning-1\">Tic-tac-toe with Q-learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Tic-tac-toe-For-The-Win-\" data-toc-modified-id=\"Tic-tac-toe-For-The-Win--3\">Tic-tac-toe For The Win </a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-4\">Code</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1937d89700a8c723",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<center><h2>Tic-tac-toe with Q-learning</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a03b93b78b0ab450",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Frame tic-tac-toe as a Reinforcement Learning problem.\n",
    "- Create an agent that learns an optimal tic-tac-toe policy through Q-learning.\n",
    "- Write Q-learning from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bae951432448099",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Tic-tac-toe For The Win \n",
    "-----\n",
    " \n",
    "1. Please start by reading the [Wikipedia article on tic-tac-toe](https://en.wikipedia.org/wiki/Tic-tac-toe) to make sure you understand the game.\n",
    "\n",
    "1. Play a couple of games against a friend to understand the game play.\n",
    "\n",
    "1. Re-read Sutton and Barto's 1.5 \"An Extended Example: Tic-Tac-Toe\".\n",
    "\n",
    "Sample state tree:\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/XoxSLlMl.png\" width=\"75%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e1ed5da72cdd6dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the game of tic-tac-toe, answer the following questions in the following cell:\n",
    "\n",
    "1. How is it sequential?\n",
    "1. What is the environment? \n",
    "1. Who is the agent?\n",
    "1. What are the states?\n",
    "1. What are the rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9975dc99a3d7a9f9",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "__Your answer below here__:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the code, answer the following questions:\n",
    "    \n",
    "1. What does epsilon do?\n",
    "1. Why is `alpha=1.0`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your answer below here__:  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a637d694fdcb2488",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Code\n",
    " ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aef5de44dbb8ee54",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7f6a4e84ed4557f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df91c7a29e5785ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TicTacToe():\n",
    "    def __init__(self):\n",
    "        self.state = '         ' # Represent state as a vector of strings (not a matrix)\n",
    "        self.player = 'X'\n",
    "        self.winner = None\n",
    "\n",
    "    def allowed_moves(self):\n",
    "        states = []\n",
    "        if self.winner:\n",
    "            return states\n",
    "        for i in range(len(self.state)):\n",
    "            if self.state[i] == ' ':\n",
    "                states.append(self.state[:i] + self.player + self.state[i+1:])\n",
    "        return states\n",
    "\n",
    "    def make_move(self, next_state):\n",
    "        if self.winner:\n",
    "            raise(Exception(\"Game already completed, cannot make another move!\"))\n",
    "        if not self.valid_move(next_state):\n",
    "            raise(Exception(\"Cannot make move {} to {} for player {}\".format(\n",
    "                    self.state, next_state, self.player)))\n",
    "\n",
    "        self.state = next_state\n",
    "        self.winner = self.predict_winner(self.state)\n",
    "        if self.winner:\n",
    "            self.player = None\n",
    "        elif self.player == 'X':\n",
    "            self.player = 'O'\n",
    "        else:\n",
    "            self.player = 'X'\n",
    "\n",
    "    def playable(self):\n",
    "        return ( (not self.winner) and any(self.allowed_moves()) )\n",
    "\n",
    "    def predict_winner(self, state):\n",
    "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "        winner = None\n",
    "        for line in lines:\n",
    "            line_state = state[line[0]] + state[line[1]] + state[line[2]]\n",
    "            if line_state == 'XXX':\n",
    "                winner = 'X'\n",
    "            elif line_state == 'OOO':\n",
    "                winner = 'O'\n",
    "        return winner\n",
    "\n",
    "    def valid_move(self, next_state):\n",
    "        allowed_moves = self.allowed_moves()\n",
    "        if any(state == next_state for state in allowed_moves):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def print_board(self):\n",
    "        s = self.state\n",
    "        print(f'     {s[0]} | {s[1]} | {s[2]} ')\n",
    "        print(f'    -----------')\n",
    "        print(f'     {s[3]} | {s[4]} | {s[5]} ')\n",
    "        print(f'    -----------')\n",
    "        print(f'     {s[6]} | {s[7]} | {s[8]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-678d24bd872d322a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"All of the Agent except Q-learning has been given to you.\"\n",
    "    def __init__(self, game_class, epsilon=0.1, alpha=1.0, value_player='X'):\n",
    "        self.V = dict() # Build up the values of different states as we encounter them; Note the Markov assumption\n",
    "        self.NewGame = game_class\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.value_player = value_player\n",
    "\n",
    "    def state_value(self, game_state):\n",
    "        \"Look up state value. If never seen state, then assume neutral.\"\n",
    "        return self.V.get(game_state, 0.0) \n",
    "\n",
    "    def learn_game(self, n_episodes=1_000):\n",
    "        \"Let's learn through complete experience to get that reward.\"\n",
    "        for episode in range(n_episodes):\n",
    "            self.learn_from_episode()\n",
    "\n",
    "    def learn_from_episode(self):\n",
    "        \"Update Values based on reward.\"\n",
    "        game = self.NewGame()\n",
    "        while game.playable():\n",
    "            self.learn_from_move(game)\n",
    "        self.V[game.state] = self.reward(game)\n",
    "        \n",
    "    def learn_select_move(self, game):\n",
    "        \"Exploration and exploitation\"\n",
    "        allowed_state_values = self.state_values( game.allowed_moves() )\n",
    "        if game.player == self.value_player:\n",
    "            best_move = self.argmax_V(allowed_state_values)\n",
    "        else:\n",
    "            best_move = self.argmin_V(allowed_state_values)\n",
    "\n",
    "        selected_move = best_move\n",
    "        if random.random() < self.epsilon:\n",
    "            selected_move = self.random_V(allowed_state_values)\n",
    "\n",
    "        return (best_move, selected_move)\n",
    "\n",
    "    def play_select_move(self, game):\n",
    "        \"Make the move based on the best option for the player.\"\n",
    "        allowed_state_values = self.state_values( game.allowed_moves() )\n",
    "        if game.player == self.value_player:\n",
    "            return self.argmax_V(allowed_state_values)\n",
    "        else:\n",
    "            return self.argmin_V(allowed_state_values)\n",
    "\n",
    "    def demo_game(self, verbose=False):\n",
    "        \"Run a game without learning.\"\n",
    "        game = self.NewGame()\n",
    "        t = 0\n",
    "        while game.playable():\n",
    "            if verbose:\n",
    "                print(\" \\nTurn {}\\n\".format(t))\n",
    "                game.print_board()\n",
    "            move = self.play_select_move(game)\n",
    "            game.make_move(move)\n",
    "            t += 1\n",
    "        if verbose:\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "        if game.winner:\n",
    "            if verbose:\n",
    "                print(\"\\n{} is the winner!\".format(game.winner))\n",
    "            return game.winner\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"\\nIt's a draw!\")\n",
    "            return '-'\n",
    "\n",
    "    def interactive_game(self, agent_player='X'):\n",
    "        \"Play in meatspace.\"\n",
    "        game = self.NewGame()\n",
    "        t = 0\n",
    "        while game.playable():\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "            if game.player == agent_player:\n",
    "                move = self.play_select_move(game)\n",
    "                game.make_move(move)\n",
    "            else:\n",
    "                move = self.request_human_move(game)\n",
    "                game.make_move(move)\n",
    "            t += 1\n",
    "\n",
    "        print(\" \\nTurn {}\\n\".format(t))\n",
    "        game.print_board()\n",
    "\n",
    "        if game.winner:\n",
    "            print(\"\\n{} is the winner!\".format(game.winner))\n",
    "            return game.winner\n",
    "        print(\"\\nIt's a draw!\")\n",
    "        return '-'\n",
    "\n",
    "    def round_V(self):\n",
    "        \"After training, this makes action selection random from equally-good choices\"\n",
    "        for k in self.V.keys():\n",
    "            self.V[k] = round(self.V[k],1)\n",
    "\n",
    "    def state_values(self, game_states):\n",
    "        return dict((state, self.state_value(state)) for state in game_states)\n",
    "\n",
    "    def argmax_V(self, state_values):\n",
    "        \"For the best possible states, chose randomly amongst them.\"\n",
    "        max_V = max(state_values.values())\n",
    "        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def argmin_V(self, state_values):\n",
    "        \"For the worst possible states, chose randomly amongst them.\"\n",
    "        min_V = min(state_values.values())\n",
    "        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def random_V(self, state_values):\n",
    "        \"Any state will do.\"\n",
    "        return random.choice(list(state_values.keys()))\n",
    "\n",
    "    def reward(self, game):\n",
    "        if game.winner == self.value_player:\n",
    "            return 1.0 # Winning is good\n",
    "        elif game.winner:\n",
    "            return -1.0 # Losing is bad\n",
    "        else:\n",
    "            return 0.0  # Tying is indifferent\n",
    "\n",
    "    def request_human_move(self, game):\n",
    "        \"Meatspace\"\n",
    "        allowed_moves = [i+1 for i in range(9) if game.state[i] == ' ']\n",
    "        human_move = None\n",
    "        while not human_move:\n",
    "            idx = int(input('Choose move for {}, from {} : '.format(game.player, allowed_moves)))\n",
    "            if any([i==idx for i in allowed_moves]):\n",
    "                human_move = game.state[:idx-1] + game.player + game.state[idx:]\n",
    "        return human_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bc7429735b92e6c4",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Write the learn_from_move method for Agent class.\n",
    "\n",
    "Each line has been started for you.\n",
    "\n",
    "No tests :(\n",
    "\"\"\"\n",
    "\n",
    "class Agent(Agent): # Note: Updated class (with same name) inherits from old class (with same name)\n",
    "    def learn_from_move(self, game):\n",
    "        \"The heart of Q-learning.\"\n",
    "        \n",
    "        # TODO: Finish each line with code and comments\n",
    "        current_state = None\n",
    "        best_next_move, selected_next_move = None, None\n",
    "        r = None\n",
    "        \n",
    "        current_state_value = None\n",
    "        best_move_value = None\n",
    "        td_target = None\n",
    "        self.V[current_state] = None # This is Q-learning. The previous lines setup this line. \n",
    "        \n",
    "        game.make_move(selected_next_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-facd8329031e20e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def print_demo_game_stats(agent, n_games=100):\n",
    "    \"Run demo to get sample statistics, no learning\"\n",
    "    long_name = {'X': 'X wins', \n",
    "               'O': 'O Wins',\n",
    "               '-': 'Tie'}\n",
    "    results = Counter([agent.demo_game() for _ in range(n_games)])\n",
    "    game_stats = {long_name[k]: v/n_games for k, v in results.items()}\n",
    "    for k in long_name.values():\n",
    "        print(f\"{k:<7}: {game_stats.get(k, 0):^8.2%}\")\n",
    "    print(\"\")\n",
    " \n",
    "def train(agent, training_block_size = 1_000, n_training_blocks = 10):\n",
    "    \"Given agent, do more training. Return (hopefully) improved agent.\"\n",
    "    print(\"Before learning:\")\n",
    "    print_demo_game_stats(agent, n_games=1_000)\n",
    "    for n_training_block in range(1, n_training_blocks+1):\n",
    "        agent.learn_game(training_block_size)\n",
    "        \n",
    "        print(f\"After {training_block_size*n_training_block:,} learning games:\")\n",
    "        print_demo_game_stats(agent)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52a8b3678487a28d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before learning:\n",
      "X wins :  56.90% \n",
      "O Wins :  31.00% \n",
      "Tie    :  12.10% \n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Cannot make move           to None for player X",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-106716b6b3b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              )\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-855879eb2675>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, training_block_size, n_training_blocks)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint_demo_game_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_training_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_training_blocks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_block_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"After {training_block_size*n_training_block:,} learning games:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f349d251c162>\u001b[0m in \u001b[0;36mlearn_game\u001b[0;34m(self, n_episodes)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m\"Let's learn through complete experience to get that reward.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_from_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn_from_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f349d251c162>\u001b[0m in \u001b[0;36mlearn_from_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_from_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-afa9e6ca0d46>\u001b[0m in \u001b[0;36mlearn_from_move\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# This is Q-learning. The previous lines setup this line.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_next_move\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-13e7fa22c6a6>\u001b[0m in \u001b[0;36mmake_move\u001b[0;34m(self, next_state)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             raise(Exception(\"Cannot make move {} to {} for player {}\".format(\n\u001b[0;32m---> 21\u001b[0;31m                     self.state, next_state, self.player)))\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Cannot make move           to None for player X"
     ]
    }
   ],
   "source": [
    "# Let's if the agent learns 🤞\n",
    "agent = Agent(TicTacToe, \n",
    "              epsilon = 0.1,\n",
    "              alpha = 1, \n",
    "             )\n",
    "agent = train(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54ad7bb6f74621af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your training should look something like this:\n",
    "\n",
    "```\n",
    "Before learning:\n",
    "X wins :  56.00% \n",
    "O Wins :  31.50% \n",
    "Tie    :  12.50% \n",
    "\n",
    "After 1,000 learning games:\n",
    "X wins :  56.00% \n",
    "O Wins :  38.00% \n",
    "Tie    :  6.00%  \n",
    "\n",
    "After 2,000 learning games:\n",
    "X wins :  53.00% \n",
    "O Wins :  29.00% \n",
    "Tie    :  18.00% \n",
    "\n",
    "After 3,000 learning games:\n",
    "X wins :  46.00% \n",
    "O Wins :  34.00% \n",
    "Tie    :  20.00% \n",
    "\n",
    "After 4,000 learning games:\n",
    "X wins :  35.00% \n",
    "O Wins :  18.00% \n",
    "Tie    :  47.00% \n",
    "\n",
    "After 5,000 learning games:\n",
    "X wins :  20.00% \n",
    "O Wins :  5.00%  \n",
    "Tie    :  75.00% \n",
    "\n",
    "After 6,000 learning games:\n",
    "X wins :  16.00% \n",
    "O Wins :  3.00%  \n",
    "Tie    :  81.00% \n",
    "\n",
    "After 7,000 learning games:\n",
    "X wins :  9.00%  \n",
    "O Wins :  2.00%  \n",
    "Tie    :  89.00% \n",
    "\n",
    "After 8,000 learning games:\n",
    "X wins :  8.00%  \n",
    "O Wins :  0.00%  \n",
    "Tie    :  92.00% \n",
    "\n",
    "After 9,000 learning games:\n",
    "X wins :  2.00%  \n",
    "O Wins :  1.00%  \n",
    "Tie    :  97.00% \n",
    "\n",
    "After 10,000 learning games:\n",
    "X wins :  0.00%  \n",
    "O Wins :  2.00%  \n",
    "Tie    :  98.00% \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d76525bc9c523c1e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Let's peak at first couple of states and their values\")\n",
    "print('State   ', '\\t', 'Value')\n",
    "for k, v in list(agent.V.items())[0:10]:\n",
    "    print(k, '\\t', v),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dde04911446d5a70",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your State/Value pairs should look something like this:\n",
    "\n",
    "```\n",
    "State    \t Value\n",
    "None \t None\n",
    "          \t 0.0\n",
    "     X    \t 0.0\n",
    "  O  X    \t 0.0\n",
    "  O XX    \t 0.0\n",
    "O O XX    \t 1.0\n",
    "OXO XX    \t 1.0\n",
    "OXOOXX    \t 1.0\n",
    "OXOOXX  X \t -1.0\n",
    "OXOOXX OX \t 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86aac898fe3992eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Demo a single game\n",
    "agent.demo_game(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
