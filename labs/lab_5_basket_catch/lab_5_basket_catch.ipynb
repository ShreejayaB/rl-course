{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4fcb23ff6bbf032",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-Q-learning-to-Play-Catch\" data-toc-modified-id=\"Deep-Q-learning-to-Play-Catch-1\">Deep Q-learning to Play Catch</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-3\">Code</a></span></li><li><span><a href=\"#Demo-A-Single-Episode-\" data-toc-modified-id=\"Demo-A-Single-Episode--4\">Demo A Single Episode </a></span></li><li><span><a href=\"#Inspect-predictions-from-the-trained-model-\" data-toc-modified-id=\"Inspect-predictions-from-the-trained-model--5\">Inspect predictions from the trained model </a></span></li><li><span><a href=\"#Hints\" data-toc-modified-id=\"Hints-6\">Hints</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-875f24e3b32f2656",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<center><h2>Deep Q-learning to Play Catch</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6e810163b8e1b57",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Understand how Deep Q-learning can learn to play Catch, a simplified version of Pong.\n",
    "- Implement the core logic of Experience Replay, including Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d3a2d2bfbbd025c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76933fe6cc8040c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "actions = [\"move left\", \"stay\", \"move right\"]\n",
    "n_actions = len(actions)\n",
    "grid_size = 3     # Start with minimum viable demo (MVD) \n",
    "basket_size = 1   # Start with minimum viable demo (MVD) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "code_folding": [],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b85f44998a0116cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Catch():\n",
    "    \"\"\"Catch is a simplfied version of Pong.\n",
    "    Catch tries to capture a single pixel “fruit” using a three pixel “basket”. \n",
    "    The fruit moves down one pixel per step.\n",
    "    Reward of +1 if it catches the fruit and -1 if it misses.\n",
    "    Input: The network sees the entire \"pixels\" grid.\n",
    "    Outputs: 3 actions (move left, stay, move right).\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size, basket_size, actions):\n",
    "        self.basket_size = basket_size \n",
    "        self.grid_size = grid_size\n",
    "        self.actions = actions\n",
    "        self.n_actions=len(actions)\n",
    "        self.empty_canvas()\n",
    "        self.reset_state() # Pick random starting location\n",
    "        self.update_canvas()\n",
    "            \n",
    "    def empty_canvas(self):\n",
    "        \"Reset to canvas empty, aka all zeros\"\n",
    "        self.canvas = np.zeros((self.grid_size,)*2)\n",
    "\n",
    "    def act(self, action=1): # Default action is to stay\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        game_over_state = self.is_over()\n",
    "        return self.observe(), reward\n",
    "\n",
    "    def is_over(self):\n",
    "        \"Fruit is at bottom.\"\n",
    "        if self.state[0] >= self.grid_size: # Check fruit row index is at bottom\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_reward(self):\n",
    "        \"Let's see if fruit is in basket or missed.\"\n",
    "        fruit_row, fruit_col, basket = self.state  #[0] # This line is tricky\n",
    "        if self.is_over():\n",
    "            if abs(fruit_col - basket) <= 1:\n",
    "                return 1 # Fruit in basket 🙂\n",
    "            else:\n",
    "                return -1 # Fruit missed basket 😦\n",
    "        else:\n",
    "            return 0 # Carry on 😐\n",
    "\n",
    "    def observe(self):\n",
    "        \"Convert internal matrix representation into a vector for the input to the MLP DL model.\"\n",
    "        return self.canvas.reshape((1, -1))\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"Pick a new starting place for fruit and basket.\"\n",
    "        n = np.random.randint(low=0, high=self.grid_size)\n",
    "        m = np.random.randint(low=0, high=self.grid_size-basket_size-1)\n",
    "        self.state = np.asarray([0,  # Row index of fruit \n",
    "                                 n,  # Col index of fruit\n",
    "                                 m]) # Col index of left side of basket (row is always bottom)\n",
    "        \n",
    "    def update_state(self, action_encoded):\n",
    "        \"Given an action, move basket and advance fruit.\"\n",
    "        # Convert encoded action into change in basket index\n",
    "        if action_encoded == 0:   # Left\n",
    "            action_idx = -1\n",
    "        elif action_encoded == 1: # Stay\n",
    "            action_idx = 0\n",
    "        else:\n",
    "            action_idx = 1        # Right\n",
    "\n",
    "        fruit_row_idx, fruit_col_idx, basket_idx = self.state\n",
    "        new_basket_idx = min(max(1, basket_idx+action_idx), self.grid_size-self.basket_size) # Basket moves\n",
    "        fruit_row_idx += 1  # Fruit falls down 1 step\n",
    "        self.state = np.asarray([fruit_row_idx, fruit_col_idx, new_basket_idx])\n",
    "        if not self.is_over():\n",
    "            self.update_canvas()\n",
    "        else:\n",
    "            self.get_reward()\n",
    "            \n",
    "    def update_canvas(self):\n",
    "        \"Read state of fruit and basket, put on canvas.\"\n",
    "        self.empty_canvas()\n",
    "        # Draw fruit\n",
    "        self.canvas[self.state[0], self.state[1]] = 1  \n",
    "        # Draw basket\n",
    "        self.canvas[-1, self.state[2]:self.state[2]+self.basket_size] = np.ones(self.basket_size) #.reshape((1, -1))  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo A Single Episode \n",
    "----\n",
    "\n",
    "Watch a single game episode to understand the game mechanics.\n",
    "\n",
    "Comment out during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c468fc6df96498d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# c = Catch(grid_size=grid_size, basket_size=basket_size, actions=actions)\n",
    "\n",
    "# while not c.is_over():\n",
    "#     reply = input(\"Press return to make a random move. Press 'q' then return to quit: \") \n",
    "#     if reply == \"q\": \n",
    "#         break\n",
    "#     print(c.canvas)                                # Show \"screen\"\n",
    "#     action = np.random.randint(0, 3)               # Randomly select\n",
    "#     canvas_snapshot, reward = c.act(action=action) # Make move and see what happens\n",
    "#     print(f\"current reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "code_folding": [],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a703e09501b31a94",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    \"Store the agent's experiences inorder to collect enough example to get a reward signal.\"\n",
    "    \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        self.memory.append([states, game_over])\n",
    "        \n",
    "        # If memory is too large, then evict to reduce memory size\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            # Evict oldest\n",
    "            del self.memory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9464a1a1d088b127",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Write the get_batch method for ExperienceReplay class.\n",
    "\n",
    "Each line has been started for you.\n",
    "\n",
    "No tests  😦\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class ExperienceReplay(ExperienceReplay): # New class (with same name) inherits everything from old class (with same name)\n",
    "    \n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        \n",
    "        # TODO: Finish each line with code and comments\n",
    "        len_memory = len(self.memory)                              # Given to you\n",
    "        n_actions = None                                           # TODO: Read from neural network model\n",
    "        env_dim =  None                                            # TODO: Read from neural network model\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))  # Given to you\n",
    "        targets = np.zeros((inputs.shape[0], n_actions))           # Given to you\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])): # Given to you\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0] # Given to you\n",
    "            game_over = self.memory[idx][1]                        # Given to you\n",
    "            inputs[i:i+1] = state_t                                # Given to you\n",
    "            targets[i] = model.predict(state_t)[0]                 # Given to you; There should be no target values for actions not taken.\n",
    "            q_sa = None                                            # TODO: Find best model prediction for state\n",
    "            if game_over:                                          # Given to you\n",
    "                targets[i, action_t] = reward_t                    # Given to you\n",
    "            else:                                                  # Given to you\n",
    "                targets[i, action_t] = None                        # TODO: Update with Q-learning\n",
    "                \n",
    "\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "code_folding": [],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f703763742b2a581",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 6)                 60        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a Deep Learning model in Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input and first hidden layer\n",
    "model.add(Dense(units=(grid_size*grid_size+n_actions)//2,  # Rough rule of thumb for hidden layer size: mean of input and output \n",
    "                input_shape=(grid_size*grid_size,), # Define by \"pixel\" space\n",
    "                activation='relu') # ReLU a common modern choice\n",
    "         ) \n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=n_actions,      # Defined by action space\n",
    "                activation='softmax') # Standard for categorial output\n",
    "         ) \n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',       # Hot rodded version of SGD\n",
    "              loss=\"categorical_crossentropy\") # Standard for categorial output\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "code_folding": [],
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6506eb61dd6d5363",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/101 | Loss value:    inf | Win count:   0\n",
      "Epoch: 010/101 | Loss value:  1.286 | Win count:  10\n",
      "Epoch: 020/101 | Loss value:  1.271 | Win count:  20\n",
      "Epoch: 030/101 | Loss value:  1.294 | Win count:  30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-60109efc3005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Get collected data to train model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Train model on experiences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-67d73aa22881>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, model, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# There should be no target values for actions not taken.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mq_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1613\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m     \"\"\"\n\u001b[0;32m-> 1615\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   3966\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3967\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3968\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   3969\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mflat_map_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FlatMapDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   1731\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Training\n",
    "\n",
    "# Define environment\n",
    "c = Catch(grid_size=grid_size, basket_size=basket_size, actions=actions)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=500)\n",
    "\n",
    "# Exploration rate\n",
    "epsilon = .1  \n",
    "\n",
    "# Training variables\n",
    "n_episodes = 101  \n",
    "#       1 is a good choice for number of episodes to seef if there is silly error\n",
    "#      11 is a good choice for number of episodes to see if model is learning\n",
    "#   3_001 is a good choice for number of episodes for complete learning\n",
    "win_count = 0\n",
    "history = []\n",
    "loss = float('inf')\n",
    "    \n",
    "for e in range(n_episodes): \n",
    "\n",
    "    if (e == 0) or (e % 10 == 0):\n",
    "        print(f\"Epoch: {e:03d}/{n_episodes:,} | Loss value: {loss:>6.3f} | Win count: {win_count:>3}\")\n",
    "        \n",
    "    # The next new episode.\n",
    "    c.reset_state()\n",
    " \n",
    "    while not c.is_over():\n",
    "        \n",
    "        # Get initial input (as vector).\n",
    "        current_screen = c.observe() \n",
    "        \n",
    "        # Get next action - You guessed it eplison-greedy.\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, n_actions, size=1)\n",
    "        else:\n",
    "            q = model.predict(current_screen)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        # Apply action, get rewards and new state.\n",
    "        future_screen, reward = c.act(action)\n",
    "        if reward == 1:\n",
    "            win_count += 1\n",
    "\n",
    "        # Store experience.\n",
    "        exp_replay.remember([current_screen, action, reward, future_screen], c.is_over())\n",
    "\n",
    "        # Get collected data to train model.\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=50)\n",
    "\n",
    "        # Train model on experiences.\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "        \n",
    "    history.append(win_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect predictions from the trained model \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def pprint_predicted_action(actions: List[str], action_index: List[float]) -> None:\n",
    "    \"Pretty print predicted action give the softmax output vector of a model\"\n",
    "    print(f\"\\nModel's predicted action: {actions[np.argmax(action_index)].title()}\")\n",
    "\n",
    "def pprint_canvas(canvas):\n",
    "    \"Pretty print canvas\"\n",
    "    print(\"Current board state:\")\n",
    "    print(canvas.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-064ed3bf4522a5a7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Make new game\n",
    "c = Catch(grid_size=grid_size, basket_size=basket_size, actions=actions)\n",
    "pprint_canvas(c.canvas)\n",
    "\n",
    "# Given a board sate, what move does the model predict?\n",
    "state = c.observe()\n",
    "pprint_predicted_action(actions, action_index=model.predict(state)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.basket_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6987c17f988f77c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "__Example of trained model on a larger grid__\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "[[0 1 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0 0 1 1 1]]\n",
    "\n",
    "Model's predicted action: Move Left\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints\n",
    "-----\n",
    "\n",
    "- The learning can be slow and unstable! Because:\n",
    "    - A simple learning mechanism \n",
    "    - A small model\n",
    "    - A naive implementation of experience replay\n",
    "- The goal of the lab is for you to gain experience implementing experience replay, not create an optimal system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b7fdd9c46de51a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
